{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8104cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from s4hci.models.noise import S4Noise\n",
    "from s4hci.models.planet import S4Planet\n",
    "from s4hci.models.normalization import S4FrameNormalization\n",
    "from s4hci.utils.adi_tools import combine_residual_stack\n",
    "from s4hci.utils.data_handling import save_as_fits, load_adi_data\n",
    "from s4hci.models.rotation import FieldRotationModel\n",
    "\n",
    "from applefy.utils.file_handling import open_fits\n",
    "from applefy.utils.fake_planets import add_fake_planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fff3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe94b6",
   "metadata": {},
   "source": [
    "# Load the data and insert the fake planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0832fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.) Load the dataset\n",
    "science_data, raw_angles, raw_psf_template_data = \\\n",
    "    load_adi_data(\n",
    "        hdf5_dataset=\"/fast/mbonse/s4/30_data/HD22049_303_199_C-0065_C_.hdf5\",\n",
    "        data_tag=\"object\",\n",
    "        psf_template_tag=\"psf_template\",\n",
    "        para_tag=\"header_object/PARANG\")\n",
    "\n",
    "science_data = science_data[:, 12:-12, 12:-12]\n",
    "raw_angles = raw_angles[:]\n",
    "\n",
    "# Background subtraction of the PSF template\n",
    "psf_template_data = np.median(raw_psf_template_data, axis=0)\n",
    "psf_template_data = psf_template_data - np.min(psf_template_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e22599ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_planet_config_file = \"/fast/mbonse/s4/70_results/07_partial_contrast_grid/HD22049_303_199_C-0065_C_/configs_cgrid/exp_ID_0083a.json\"\n",
    "with open(fake_planet_config_file) as json_file:\n",
    "    fake_planet_config = json.load(json_file)\n",
    "\n",
    "data_with_fake_planet = add_fake_planets(\n",
    "    input_stack=science_data,\n",
    "    psf_template=psf_template_data,\n",
    "    parang=raw_angles,\n",
    "    dit_psf_template=0.004256,\n",
    "    dit_science=0.08,\n",
    "    experiment_config=fake_planet_config,\n",
    "    scaling_factor=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3471738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the data to speed up the training\n",
    "data_with_fake_planet_cut = data_with_fake_planet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d50afd",
   "metadata": {},
   "source": [
    "# Implement the current training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5deb5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotationS4:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        science_data,\n",
    "        psf_template,\n",
    "        lambda_reg,\n",
    "        parang,\n",
    "        work_dir=None,\n",
    "        psf_cut_radius=4.0,\n",
    "        mask_radius=5.5):\n",
    "\n",
    "        # 1.) Save the data\n",
    "        self.device = 0\n",
    "        self.science_data = torch.from_numpy(science_data).float()\n",
    "        self.psf_template = psf_template\n",
    "        self.data_image_size = self.science_data.shape[-1]\n",
    "        self.parang = parang\n",
    "        \n",
    "        if work_dir is not None:\n",
    "            self.work_dir = Path(work_dir)\n",
    "        else:\n",
    "            self.work_dir = None\n",
    "        self.residuals_dir, self.tensorboard_dir, self.models_dir = \\\n",
    "            self._setup_working_dir()\n",
    "\n",
    "        # 2.) Create the noise model\n",
    "        self.noise_model = S4Noise(\n",
    "            data_image_size=self.data_image_size,\n",
    "            psf_template=psf_template,\n",
    "            lambda_reg=lambda_reg,\n",
    "            cut_radius_psf=psf_cut_radius,\n",
    "            mask_template_setup=(\"radius\", mask_radius),\n",
    "            convolve=True,\n",
    "            verbose=True).float()\n",
    "        \n",
    "        # 2.) Create the filed rotation model\n",
    "        self.rotation_model = FieldRotationModel(\n",
    "            all_angles=parang,\n",
    "            input_size=self.data_image_size,\n",
    "            subsample=1,\n",
    "            inverse=False,\n",
    "            register_grid=True)\n",
    "        \n",
    "        self.inverse_rotation_model = FieldRotationModel(\n",
    "            all_angles=parang,\n",
    "            input_size=self.data_image_size,\n",
    "            subsample=1,\n",
    "            inverse=True,\n",
    "            register_grid=True)\n",
    "\n",
    "        # 3.) Create normalization model\n",
    "        self.normalization_model = S4FrameNormalization(\n",
    "            image_size=self.data_image_size,\n",
    "            normalization_type=\"normal\")\n",
    "\n",
    "        self.normalization_model.prepare_normalization(\n",
    "            science_data=self.science_data)\n",
    "        \n",
    "        # 4.) For tensorboard\n",
    "        self.tensorboard_logger = None\n",
    "        self.fine_tune_start_time = None\n",
    "        \n",
    "    def _setup_working_dir(self):\n",
    "        if self.work_dir is None:\n",
    "            return None, None, None\n",
    "\n",
    "        # make sure the working dir is a dir\n",
    "        self.work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        residuals_dir = self.work_dir / \"residuals\"\n",
    "        tensorboard_dir = self.work_dir / \"tensorboard\"\n",
    "        models_dir = self.work_dir / \"models\"\n",
    "\n",
    "        residuals_dir.mkdir(exist_ok=True)\n",
    "        tensorboard_dir.mkdir(exist_ok=True)\n",
    "        models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        return residuals_dir, tensorboard_dir, models_dir\n",
    "        \n",
    "    def _create_tensorboard_logger(self):\n",
    "        time_str = datetime.now().strftime(\"%Y-%m-%d-%Hh%Mm%Ss\")\n",
    "        self.fine_tune_start_time = time_str\n",
    "        current_logdir = self.tensorboard_dir / \\\n",
    "            Path(self.fine_tune_start_time)\n",
    "        current_logdir.mkdir()\n",
    "        self.tensorboard_logger = SummaryWriter(current_logdir)\n",
    "        \n",
    "    def _logg_loss_values(\n",
    "            self,\n",
    "            epoch,\n",
    "            loss_recon,\n",
    "            loss_reg):\n",
    "\n",
    "        if self.work_dir is None:\n",
    "            return\n",
    "\n",
    "        self.tensorboard_logger.add_scalar(\n",
    "            \"Loss/Reconstruction_loss\",\n",
    "            loss_recon,\n",
    "            epoch)\n",
    "\n",
    "        self.tensorboard_logger.add_scalar(\n",
    "            \"Loss/Regularization_loss\",\n",
    "            loss_reg,\n",
    "            epoch)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _normalize_for_tensorboard(frame_in):\n",
    "        image_for_tb = deepcopy(frame_in)\n",
    "        image_for_tb -= np.min(image_for_tb)\n",
    "        image_for_tb /= np.max(image_for_tb)\n",
    "        return image_for_tb\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def compute_residual(\n",
    "            self,\n",
    "            num_cpus=8,\n",
    "            subtract_temporal_average=False\n",
    "    ):\n",
    "        # 1.) normalize and reshape the data\n",
    "        x_norm = self.normalization_model(self.science_data)\n",
    "        science_norm_flatten = x_norm.view(x_norm.shape[0], -1)\n",
    "\n",
    "        # 2.) compute the noise estimate\n",
    "        noise_estimate = self.noise_model(science_norm_flatten)\n",
    "\n",
    "        # 3.) compute and reshape the residual sequence\n",
    "        residual_sequence = science_norm_flatten - noise_estimate\n",
    "        residual_stack = residual_sequence.view(\n",
    "            self.science_data.shape[0],\n",
    "            self.noise_model.image_size,\n",
    "            self.noise_model.image_size).detach().cpu().numpy()\n",
    "\n",
    "        # 4.) derotate and stack the residual sequence\n",
    "        residual_image = combine_residual_stack(\n",
    "            residual_stack=residual_stack,\n",
    "            angles=self.parang,\n",
    "            combine=\"mean\",\n",
    "            subtract_temporal_average=subtract_temporal_average,\n",
    "            num_cpus=num_cpus)\n",
    "\n",
    "        return residual_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c055c",
   "metadata": {},
   "source": [
    "# Implement LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88e6d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b46019e217f4c2787013a8b3a14196a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.91 GiB. GPU 0 has a total capacty of 93.23 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 91.71 GiB memory in use. Of the allocated memory 82.01 GiB is allocated by PyTorch, and 8.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 104\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_closure\u001b[39m():\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m full_closure()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 104\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_closure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# 4.) Track the current loss\u001b[39;00m\n\u001b[1;32m    106\u001b[0m current_loss, var_planet, residual_raw, residual_final \u001b[38;5;241m=\u001b[39m full_closure()\n",
      "File \u001b[0;32m/lustre/home/mbonse/2023_s4/50_code/venv_s4/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/home/mbonse/2023_s4/50_code/venv_s4/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/home/mbonse/2023_s4/50_code/venv_s4/lib/python3.10/site-packages/torch/optim/lbfgs.py:438\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 438\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    439\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    440\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m/lustre/home/mbonse/2023_s4/50_code/venv_s4/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 101\u001b[0m, in \u001b[0;36mloss_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_closure\u001b[39m():\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfull_closure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 96\u001b[0m, in \u001b[0;36mfull_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# 3.) Backward\u001b[39;00m\n\u001b[1;32m     95\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_recon \u001b[38;5;241m+\u001b[39m loss_reg\n\u001b[0;32m---> 96\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, torch\u001b[38;5;241m.\u001b[39mvar(noise_of_planet_rot, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m],\\\n\u001b[1;32m     98\u001b[0m     current_residual\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m], final_residual\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/lustre/home/mbonse/2023_s4/50_code/venv_s4/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/home/mbonse/2023_s4/50_code/venv_s4/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.91 GiB. GPU 0 has a total capacty of 93.23 GiB of which 1.52 GiB is free. Including non-PyTorch memory, this process has 91.71 GiB memory in use. Of the allocated memory 82.01 GiB is allocated by PyTorch, and 8.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = 0\n",
    "batch_size=-1\n",
    "num_epochs=300\n",
    "\n",
    "# Create the model\n",
    "s4_model = RotationS4(\n",
    "    science_data=data_with_fake_planet_cut,\n",
    "    psf_template=psf_template_data,\n",
    "    lambda_reg=0,\n",
    "    work_dir=\"/fast/mbonse/s4/70_results/09_new_rotation_loss/implementation/\",\n",
    "    parang=raw_angles)\n",
    "\n",
    "# Create the tensorboard logger\n",
    "if s4_model.work_dir is not None:\n",
    "    s4_model._create_tensorboard_logger()\n",
    "\n",
    "# 1.) normalize the science data\n",
    "x_norm = s4_model.normalization_model(s4_model.science_data)\n",
    "science_norm_flatten = x_norm.view(x_norm.shape[0], -1)\n",
    "\n",
    "# 2.) move models to the GPU\n",
    "s4_model.noise_model = s4_model.noise_model.to(device)\n",
    "s4_model.rotation_model = s4_model.rotation_model.to(device)\n",
    "s4_model.inverse_rotation_model = s4_model.inverse_rotation_model.to(device)\n",
    "science_norm_flatten = science_norm_flatten.to(device)\n",
    "\n",
    "# 3.) Create the optimizer and add the parameters we want to optimize\n",
    "optimizer = optim.LBFGS(\n",
    "    [s4_model.noise_model.betas_raw, ],\n",
    "    max_iter=5,\n",
    "    history_size=5)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    science_norm_flatten,\n",
    "    batch_size=x_norm.shape[0],\n",
    "    shuffle=False)\n",
    "\n",
    "# 5.) Run the fine-tuning\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    for tmp_frames in data_loader:\n",
    "        \n",
    "        def full_closure():\n",
    "            optimizer.zero_grad()\n",
    "            # 1.) run the forward path of the noise model\n",
    "            s4_model.noise_model.compute_betas()\n",
    "            noise_estimate = s4_model.noise_model(tmp_frames)\n",
    "\n",
    "            # 2.) compute the residual and rotate it\n",
    "            residual_sequence = tmp_frames - noise_estimate\n",
    "            residual_sequence = residual_sequence.view(\n",
    "                residual_sequence.shape[0],\n",
    "                1,\n",
    "                s4_model.data_image_size,\n",
    "                s4_model.data_image_size)\n",
    "\n",
    "            rotated_frames = s4_model.rotation_model(\n",
    "                residual_sequence,\n",
    "                parang_idx=torch.arange(len(residual_sequence)))\n",
    "            \n",
    "            # 3.) Compute the current residual\n",
    "            current_residual = torch.mean(rotated_frames, axis=0)\n",
    "            \n",
    "            # 4.) Compute the effect of the planet on the noise\n",
    "            current_residual_pos = current_residual.clone()\n",
    "            current_residual_pos[current_residual_pos<0] = 0\n",
    "            current_planet = current_residual_pos.repeat(len(residual_sequence), 1, 1).clone()\n",
    "            \n",
    "            current_planet = s4_model.inverse_rotation_model(\n",
    "                current_planet.unsqueeze(1),\n",
    "                parang_idx=torch.arange(len(residual_sequence)))\n",
    "            \n",
    "            current_planet = current_planet.view(len(residual_sequence), -1)\n",
    "            \n",
    "            noise_of_planet = s4_model.noise_model(current_planet)\n",
    "            planet_noise_sequence = noise_of_planet.view(\n",
    "                noise_of_planet.shape[0],\n",
    "                1,\n",
    "                s4_model.data_image_size,\n",
    "                s4_model.data_image_size)\n",
    "\n",
    "            noise_of_planet_rot = s4_model.rotation_model(\n",
    "                planet_noise_sequence,\n",
    "                parang_idx=torch.arange(len(residual_sequence)))\n",
    "\n",
    "            final_residual = torch.mean(rotated_frames - noise_of_planet_rot, axis=0)\n",
    "            \n",
    "            # 2.) Compute the loss\n",
    "            loss_recon = torch.var(rotated_frames - noise_of_planet_rot, axis=0).sum()\n",
    "            loss_reg = (s4_model.noise_model.betas_raw ** 2).sum() \\\n",
    "                * s4_model.noise_model.lambda_reg \\\n",
    "                 / rotated_frames.shape[0]\n",
    "\n",
    "            # 3.) Backward\n",
    "            loss = loss_recon + loss_reg\n",
    "            loss.backward()\n",
    "            return loss, torch.var(noise_of_planet_rot, axis=0).detach().cpu().numpy()[0],\\\n",
    "                current_residual.detach().cpu().numpy()[0], final_residual.detach().cpu().numpy()[0]\n",
    "        \n",
    "        def loss_closure():\n",
    "            return full_closure()[0]\n",
    "\n",
    "\n",
    "        optimizer.step(loss_closure)\n",
    "        # 4.) Track the current loss\n",
    "        current_loss, var_planet, residual_raw, residual_final = full_closure()\n",
    "\n",
    "    # 5.) Logg the information\n",
    "    s4_model._logg_loss_values(\n",
    "        epoch=epoch,\n",
    "        loss_recon=current_loss,\n",
    "        loss_reg=0)\n",
    "    \n",
    "    s4_model.tensorboard_logger.add_image(\n",
    "        \"Images/Residual_Raw\",\n",
    "        s4_model._normalize_for_tensorboard(residual_raw),\n",
    "        epoch,\n",
    "        dataformats=\"HW\")\n",
    "\n",
    "    s4_model.tensorboard_logger.add_image(\n",
    "        \"Images/Residual_Final\",\n",
    "        s4_model._normalize_for_tensorboard(residual_final),\n",
    "        epoch,\n",
    "        dataformats=\"HW\")\n",
    "\n",
    "    s4_model.tensorboard_logger.add_image(\n",
    "        \"Images/Planet_var\",\n",
    "        s4_model._normalize_for_tensorboard(var_planet),\n",
    "        epoch,\n",
    "        dataformats=\"HW\")\n",
    "\n",
    "# 7.) Clean up GPU\n",
    "s4_model.noise_model = s4_model.noise_model.cpu()\n",
    "s4_model.rotation_model = s4_model.rotation_model.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85285e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dbaeb7f",
   "metadata": {},
   "source": [
    "# Save fits files later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "        tmp_residual_dir = s4_model.residuals_dir / \\\n",
    "            Path(s4_model.fine_tune_start_time)\n",
    "        tmp_residual_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        save_as_fits(\n",
    "            tmp_residual,\n",
    "            tmp_residual_dir /\n",
    "            Path(\"Residual_epoch_\" + str(epoch).zfill(4)\n",
    "                 + \".fits\"),\n",
    "            overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf19b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
