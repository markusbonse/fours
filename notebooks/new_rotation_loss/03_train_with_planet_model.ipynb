{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8104cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from s4hci.models.noise import S4Noise\n",
    "from s4hci.models.planet import S4Planet\n",
    "from s4hci.models.normalization import S4FrameNormalization\n",
    "from s4hci.utils.adi_tools import combine_residual_stack\n",
    "from s4hci.utils.data_handling import save_as_fits, load_adi_data\n",
    "from s4hci.models.rotation import FieldRotationModel\n",
    "\n",
    "from applefy.utils.file_handling import open_fits\n",
    "from applefy.utils.fake_planets import add_fake_planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fff3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe94b6",
   "metadata": {},
   "source": [
    "# Load the data and insert the fake planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0832fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.) Load the dataset\n",
    "science_data, raw_angles, raw_psf_template_data = \\\n",
    "    load_adi_data(\n",
    "        hdf5_dataset=\"/fast/mbonse/s4/30_data/HD22049_303_199_C-0065_C_.hdf5\",\n",
    "        data_tag=\"object\",\n",
    "        psf_template_tag=\"psf_template\",\n",
    "        para_tag=\"header_object/PARANG\")\n",
    "\n",
    "science_data = science_data[:, 12:-12, 12:-12]\n",
    "raw_angles = raw_angles[:]\n",
    "\n",
    "# Background subtraction of the PSF template\n",
    "psf_template_data = np.median(raw_psf_template_data, axis=0)\n",
    "psf_template_data = psf_template_data - np.min(psf_template_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d50afd",
   "metadata": {},
   "source": [
    "# Implement the current training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5deb5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4_with_Planet:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        science_data,\n",
    "        psf_template,\n",
    "        lambda_reg,\n",
    "        parang,\n",
    "        conv=False,\n",
    "        work_dir=None,\n",
    "        psf_cut_radius=4.0,\n",
    "        mask_radius=5.5):\n",
    "\n",
    "        # 1.) Save the data\n",
    "        self.device = 0\n",
    "        self.science_data = torch.from_numpy(science_data).float()\n",
    "        self.psf_template = psf_template\n",
    "        self.data_image_size = self.science_data.shape[-1]\n",
    "        self.parang = parang\n",
    "        \n",
    "        if work_dir is not None:\n",
    "            self.work_dir = Path(work_dir)\n",
    "        else:\n",
    "            self.work_dir = None\n",
    "        self.residuals_dir, self.tensorboard_dir, self.models_dir = \\\n",
    "            self._setup_working_dir()\n",
    "\n",
    "        # 2.) Create the noise model\n",
    "        self.noise_model = S4Noise(\n",
    "            data_image_size=self.data_image_size,\n",
    "            psf_template=psf_template,\n",
    "            lambda_reg=lambda_reg,\n",
    "            cut_radius_psf=psf_cut_radius,\n",
    "            mask_template_setup=(\"radius\", mask_radius),\n",
    "            convolve=conv,\n",
    "            verbose=True).float()\n",
    "        \n",
    "        # # 3.) Create the planet model\n",
    "        self.planet_model = S4Planet(\n",
    "            data_image_size=self.data_image_size,\n",
    "            psf_template=self.psf_template,\n",
    "            convolve_second=True,\n",
    "            inner_mask_radius=0,\n",
    "            use_up_sample=1).float()\n",
    "\n",
    "        # 3.) Create normalization model\n",
    "        self.normalization_model = S4FrameNormalization(\n",
    "            image_size=self.data_image_size,\n",
    "            normalization_type=\"normal\")\n",
    "\n",
    "        self.normalization_model.prepare_normalization(\n",
    "            science_data=self.science_data)\n",
    "        \n",
    "        # 4.) For tensorboard\n",
    "        self.tensorboard_logger = None\n",
    "        self.fine_tune_start_time = None\n",
    "        \n",
    "    def _setup_working_dir(self):\n",
    "        if self.work_dir is None:\n",
    "            return None, None, None\n",
    "\n",
    "        # make sure the working dir is a dir\n",
    "        self.work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        residuals_dir = self.work_dir / \"residuals\"\n",
    "        tensorboard_dir = self.work_dir / \"tensorboard\"\n",
    "        models_dir = self.work_dir / \"models\"\n",
    "\n",
    "        residuals_dir.mkdir(exist_ok=True)\n",
    "        tensorboard_dir.mkdir(exist_ok=True)\n",
    "        models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        return residuals_dir, tensorboard_dir, models_dir\n",
    "        \n",
    "    def _create_tensorboard_logger(self):\n",
    "        time_str = datetime.now().strftime(\"%Y-%m-%d-%Hh%Mm%Ss\")\n",
    "        self.fine_tune_start_time = time_str\n",
    "        current_logdir = self.tensorboard_dir / \\\n",
    "            Path(self.fine_tune_start_time)\n",
    "        current_logdir.mkdir()\n",
    "        self.tensorboard_logger = SummaryWriter(current_logdir)\n",
    "        \n",
    "    def _logg_loss_values(\n",
    "            self,\n",
    "            epoch,\n",
    "            loss_recon,\n",
    "            loss_reg):\n",
    "\n",
    "        if self.work_dir is None:\n",
    "            return\n",
    "\n",
    "        self.tensorboard_logger.add_scalar(\n",
    "            \"Loss/Reconstruction_loss\",\n",
    "            loss_recon,\n",
    "            epoch)\n",
    "\n",
    "        self.tensorboard_logger.add_scalar(\n",
    "            \"Loss/Regularization_loss\",\n",
    "            loss_reg,\n",
    "            epoch)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _normalize_for_tensorboard(frame_in):\n",
    "        image_for_tb = deepcopy(frame_in)\n",
    "        image_for_tb -= np.min(image_for_tb)\n",
    "        image_for_tb /= np.max(image_for_tb)\n",
    "        return image_for_tb\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def compute_residual(\n",
    "            self,\n",
    "            num_cpus=8\n",
    "    ):\n",
    "        # 1.) Get the current planet signal and subtract it if requested\n",
    "        torch.from_numpy(np.arange(self.science_data.shape[0]))\n",
    "        planet_signal = self.planet_model.forward(planet_model_idx).squeeze(1)\n",
    "        \n",
    "        # 2.) normalize the science data\n",
    "        data_no_planet = self.science_data - planet_signal\n",
    "        tmp_mean = torch.mean(data_no_planet, axis=0)\n",
    "        tmp_std = torch.std(data_no_planet, axis=0)\n",
    "        \n",
    "        x_norm = (s4_model.science_data - tmp_mean) / tmp_std\n",
    "        science_norm_flatten = x_norm.view(x_norm.shape[0], -1)\n",
    "\n",
    "        # 3.) run the forward path with the noise model\n",
    "        s4_model.noise_model.compute_betas()\n",
    "        noise_estimate = self.noise_model(science_norm_flatten)\n",
    "        \n",
    "        # 7.) compute the residual sequence\n",
    "        residual_sequence = science_norm_flatten - noise_estimate\n",
    "        residual_stack = residual_sequence.view(\n",
    "            self.science_data.shape[0],\n",
    "            self.noise_model.image_size,\n",
    "            self.noise_model.image_size).detach().cpu().numpy()\n",
    "\n",
    "        residual_image = combine_residual_stack(\n",
    "            residual_stack=residual_stack,\n",
    "            angles=self.parang,\n",
    "            combine=\"mean\",\n",
    "            subtract_temporal_average=False,\n",
    "            num_cpus=num_cpus)\n",
    "\n",
    "        return residual_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c055c",
   "metadata": {},
   "source": [
    "# Implement the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac13460",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"0105c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b102cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the fake planet\n",
    "fake_planet_config_file = \"/fast/mbonse/s4/70_results/07_partial_contrast_grid/HD22049_303_199_C-0065_C_/configs_cgrid/exp_ID_\" + dataset_id + \".json\"\n",
    "with open(fake_planet_config_file) as json_file:\n",
    "    fake_planet_config = json.load(json_file)\n",
    "\n",
    "data_with_fake_planet = add_fake_planets(\n",
    "    input_stack=science_data,\n",
    "    psf_template=psf_template_data,\n",
    "    parang=raw_angles,\n",
    "    dit_psf_template=0.004256,\n",
    "    dit_science=0.08,\n",
    "    experiment_config=fake_planet_config,\n",
    "    scaling_factor=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9ae740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking\n",
    "ss = 5\n",
    "angles_stacked = np.array([np.mean(i) for i in np.array_split(raw_angles, int(len(raw_angles) / ss))])\n",
    "science_with_planet_stacked = np.array([np.mean(i, axis=0) for i in np.array_split(data_with_fake_planet, int(len(raw_angles) / ss))])\n",
    "science_stacked = np.array([np.mean(i, axis=0) for i in np.array_split(science_data, int(len(raw_angles) / ss))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8131df62",
   "metadata": {},
   "source": [
    "## The training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5850172",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = True\n",
    "mask_size = 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99232677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7958a0d64fdc4015a295616d4786060e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'S4_with_Planet' object has no attribute 'rotation_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# 7.) Clean up GPU\u001b[39;00m\n\u001b[1;32m    139\u001b[0m s4_model\u001b[38;5;241m.\u001b[39mnoise_model \u001b[38;5;241m=\u001b[39m s4_model\u001b[38;5;241m.\u001b[39mnoise_model\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m--> 140\u001b[0m s4_model\u001b[38;5;241m.\u001b[39mrotation_model \u001b[38;5;241m=\u001b[39m \u001b[43ms4_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotation_model\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    141\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'S4_with_Planet' object has no attribute 'rotation_model'"
     ]
    }
   ],
   "source": [
    "device = 0\n",
    "num_epochs=100\n",
    "lambda_reg=1000\n",
    "    \n",
    "# Create the model\n",
    "s4_model = S4_with_Planet(\n",
    "    science_data=science_with_planet_stacked,\n",
    "    psf_template=psf_template_data,\n",
    "    lambda_reg=lambda_reg,\n",
    "    conv=conv,\n",
    "    mask_radius=mask_size,\n",
    "    work_dir=\"/fast/mbonse/s4/70_results/09_new_rotation_loss/lbfgs_with_planet/\",\n",
    "    parang=angles_stacked)\n",
    "\n",
    "# Create the tensorboard logger\n",
    "if s4_model.work_dir is not None:\n",
    "    s4_model._create_tensorboard_logger()\n",
    "\n",
    "s4_model.planet_model.setup_for_training(\n",
    "    all_angles=s4_model.parang,\n",
    "    rotation_grid_down_sample=1,\n",
    "    upload_rotation_grid=True)\n",
    "\n",
    "# 2.) move models to the GPU\n",
    "s4_model.noise_model = s4_model.noise_model.to(device)\n",
    "s4_model.planet_model = s4_model.planet_model.to(device)\n",
    "s4_model.science_data = s4_model.science_data.to(device)\n",
    "planet_model_idx = torch.from_numpy(np.arange(s4_model.science_data.shape[0]))\n",
    "\n",
    "# 3.) Create the optimizer and add the parameters we want to optimize\n",
    "optimizer = optim.LBFGS(\n",
    "    [s4_model.noise_model.betas_raw, \n",
    "     s4_model.planet_model.planet_model],\n",
    "     max_iter=20,\n",
    "     history_size=5)\n",
    "\n",
    "# 5.) Run the fine-tuning\n",
    "# needed for gradient accumulation in order to normalize the loss\n",
    "num_steps_per_epoch = 1\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    def loss_closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1.) Get the current planet signal estimate\n",
    "        planet_signal = s4_model.planet_model.forward(planet_model_idx).squeeze(1)\n",
    "        \n",
    "        # 2.) normalize the science data\n",
    "        data_no_planet = s4_model.science_data - planet_signal.detach()\n",
    "        tmp_mean = torch.mean(data_no_planet, axis=0)\n",
    "        tmp_std = torch.std(data_no_planet, axis=0)\n",
    "        \n",
    "        x_norm = (s4_model.science_data - tmp_mean) / tmp_std\n",
    "        science_norm_flatten = x_norm.view(x_norm.shape[0], -1)\n",
    "\n",
    "        # 3.) normalize and reshape the planet signal\n",
    "        planet_signal_norm = planet_signal / tmp_std\n",
    "        planet_signal_norm = planet_signal_norm.view(\n",
    "            planet_signal.shape[0], -1)\n",
    "\n",
    "        # 3.) run the forward path with the noise model\n",
    "        s4_model.noise_model.compute_betas()\n",
    "        noise_estimate = s4_model.noise_model(science_norm_flatten)\n",
    "\n",
    "        # 4.) compute the loss\n",
    "        loss_recon = ((noise_estimate - (science_norm_flatten -  planet_signal_norm)) ** 2).sum()\n",
    "        loss_reg = (s4_model.noise_model.betas_raw ** 2).sum() \\\n",
    "            * s4_model.noise_model.lambda_reg \\\n",
    "            / num_steps_per_epoch / planet_signal.shape[0]\n",
    "\n",
    "        # 3.) Backward\n",
    "        loss = loss_recon + loss_reg\n",
    "        loss.backward()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(loss_closure)\n",
    "    # 4.) Track the current loss\n",
    "    current_loss = loss_closure()\n",
    "\n",
    "    # 5.) Logg the information\n",
    "    s4_model._logg_loss_values(\n",
    "        epoch=epoch,\n",
    "        loss_recon=current_loss,\n",
    "        loss_reg=0)\n",
    "\n",
    "    if epoch % 10==1:\n",
    "        s4_model.planet_model = s4_model.planet_model.cpu()\n",
    "        s4_model.noise_model = s4_model.noise_model.cpu()\n",
    "        s4_model.science_data = s4_model.science_data.cpu()\n",
    "        \n",
    "        tmp_residual_dir = s4_model.residuals_dir / \\\n",
    "            Path(s4_model.fine_tune_start_time)\n",
    "        tmp_residual_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # save planet signal\n",
    "        planet_signal = s4_model.planet_model(torch.tensor([0]))\n",
    "        planet_signal = s4_model.normalization_model.normalize_data(\n",
    "            planet_signal,\n",
    "            re_center=False)\n",
    "\n",
    "        tmp_frame = planet_signal[0, 0].cpu().detach().numpy()\n",
    "\n",
    "        s4_model.tensorboard_logger.add_image(\n",
    "            \"Images/Planet_signal_estimate\",\n",
    "            s4_model._normalize_for_tensorboard(tmp_frame),\n",
    "            epoch,\n",
    "            dataformats=\"HW\")\n",
    "\n",
    "        save_as_fits(\n",
    "            tmp_frame,\n",
    "            tmp_residual_dir /\n",
    "            Path(\"Planet_signal_estimate_epoch_\" + str(epoch).zfill(4)\n",
    "                 + \".fits\"),\n",
    "            overwrite=True)\n",
    "        \n",
    "        # save residual\n",
    "        residual = s4_model.compute_residual()\n",
    "        \n",
    "        s4_model.tensorboard_logger.add_image(\n",
    "            \"Images/Resiudal\",\n",
    "            s4_model._normalize_for_tensorboard(residual),\n",
    "            epoch,\n",
    "            dataformats=\"HW\")\n",
    "\n",
    "        save_as_fits(\n",
    "            residual,\n",
    "            tmp_residual_dir /\n",
    "            Path(\"Resiudal_epoch_\" + str(epoch).zfill(4)\n",
    "                 + \".fits\"),\n",
    "            overwrite=True)\n",
    "        \n",
    "        s4_model.planet_model = s4_model.planet_model.to(device)\n",
    "        s4_model.noise_model = s4_model.noise_model.to(device)\n",
    "        s4_model.science_data = s4_model.science_data.to(device)\n",
    "\n",
    "# 7.) Clean up GPU\n",
    "s4_model.noise_model = s4_model.noise_model.cpu()\n",
    "s4_model.rotation_model = s4_model.rotation_model.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.) run the forward path of the noise model\n",
    "        s4_model.noise_model.compute_betas()\n",
    "        noise_estimate = s4_model.noise_model(science_norm_flatten)\n",
    "\n",
    "        # 2.) compute the residual and rotate it\n",
    "        residual_sequence = science_norm_flatten - noise_estimate\n",
    "        residual_sequence = residual_sequence.view(\n",
    "            residual_sequence.shape[0],\n",
    "            1,\n",
    "            s4_model.data_image_size,\n",
    "            s4_model.data_image_size)\n",
    "\n",
    "        rotated_frames = s4_model.rotation_model(\n",
    "            residual_sequence,\n",
    "            parang_idx=torch.arange(len(residual_sequence)))\n",
    "\n",
    "        # 2.) Compute the loss\n",
    "        loss_recon = torch.var(rotated_frames, axis=0).sum()\n",
    "\n",
    "        loss_reg = (s4_model.noise_model.betas_raw ** 2).sum() \\\n",
    "            * s4_model.noise_model.lambda_reg \\\n",
    "            / num_steps_per_epoch / rotated_frames.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
